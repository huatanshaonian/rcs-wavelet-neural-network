"""
AutoEncoderå¯¹æ¯”åˆ†æç³»ç»Ÿ
æ”¯æŒå°æ³¢å¢å¼ºæ¨¡å¼ vs ç›´æ¥æ¨¡å¼çš„å…¨é¢å¯¹æ¯”
"""

import torch
import torch.nn as nn
import numpy as np
import time
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import mean_squared_error, mean_absolute_error
import seaborn as sns

# å¯¼å…¥ä¸¤ç§AutoEncoder
from ..models.cnn_autoencoder import WaveletAutoEncoder
from ..models.direct_autoencoder import DirectAutoEncoder
from ..models.cnn_autoencoder import ParameterMapper
from .wavelet_transform import WaveletTransform


class AutoEncoderComparator:
    """AutoEncoderå¯¹æ¯”åˆ†æå™¨"""

    def __init__(self,
                 wavelet_system: Dict[str, Any],
                 direct_system: Dict[str, Any]):
        """
        åˆå§‹åŒ–å¯¹æ¯”åˆ†æå™¨

        Args:
            wavelet_system: å°æ³¢å¢å¼ºAutoEncoderç³»ç»Ÿ
            direct_system: ç›´æ¥AutoEncoderç³»ç»Ÿ
        """
        self.wavelet_system = wavelet_system
        self.direct_system = direct_system

        # è®¾å¤‡è®¾ç½®
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        # ç»“æœå­˜å‚¨
        self.comparison_results = {}

    def compare_performance(self, test_data: Dict[str, np.ndarray],
                          batch_size: int = 10) -> Dict[str, Any]:
        """
        æ€§èƒ½å¯¹æ¯”åˆ†æ

        Args:
            test_data: æµ‹è¯•æ•°æ® {'rcs_data': [...], 'param_data': [...]}
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            performance_results: æ€§èƒ½å¯¹æ¯”ç»“æœ
        """
        print("ğŸ”¬ å¼€å§‹æ€§èƒ½å¯¹æ¯”åˆ†æ...")

        results = {
            'wavelet_mode': {},
            'direct_mode': {},
            'comparison': {}
        }

        rcs_data = test_data['rcs_data']
        param_data = test_data['param_data']

        # æµ‹è¯•å°æ³¢å¢å¼ºæ¨¡å¼
        print("  ğŸ“Š æµ‹è¯•å°æ³¢å¢å¼ºæ¨¡å¼...")
        wavelet_results = self._evaluate_system(
            self.wavelet_system, rcs_data, param_data,
            batch_size, mode='wavelet'
        )
        results['wavelet_mode'] = wavelet_results

        # æµ‹è¯•ç›´æ¥æ¨¡å¼
        print("  ğŸ“Š æµ‹è¯•ç›´æ¥æ¨¡å¼...")
        direct_results = self._evaluate_system(
            self.direct_system, rcs_data, param_data,
            batch_size, mode='direct'
        )
        results['direct_mode'] = direct_results

        # å¯¹æ¯”åˆ†æ
        print("  ğŸ“Š è®¡ç®—å¯¹æ¯”æŒ‡æ ‡...")
        results['comparison'] = self._calculate_comparison_metrics(
            wavelet_results, direct_results
        )

        self.comparison_results['performance'] = results
        return results

    def _evaluate_system(self, system: Dict[str, Any],
                        rcs_data: np.ndarray, param_data: np.ndarray,
                        batch_size: int, mode: str) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªç³»ç»Ÿ"""
        autoencoder = system['autoencoder']
        parameter_mapper = system['parameter_mapper']
        wavelet_transform = system.get('wavelet_transform', None)

        # ç§»åŠ¨åˆ°è®¾å¤‡
        autoencoder.to(self.device).eval()
        parameter_mapper.to(self.device).eval()

        # æµ‹è¯•æ•°æ®å‡†å¤‡
        test_size = min(len(rcs_data), 50)  # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°
        test_rcs = rcs_data[:test_size]
        test_params = param_data[:test_size]

        # æ€§èƒ½æŒ‡æ ‡
        reconstruction_errors = []
        prediction_errors = []
        latent_vectors = []
        inference_times = []

        # æ‰¹æ¬¡æµ‹è¯•
        with torch.no_grad():
            for i in range(0, test_size, batch_size):
                end_idx = min(i + batch_size, test_size)
                batch_rcs = torch.FloatTensor(test_rcs[i:end_idx]).to(self.device)
                batch_params = torch.FloatTensor(test_params[i:end_idx]).to(self.device)

                # æµ‹è¯•é‡å»ºèƒ½åŠ›
                start_time = time.time()

                if mode == 'wavelet':
                    # å°æ³¢æ¨¡å¼ï¼šRCS â†’ å°æ³¢ç³»æ•° â†’ AE â†’ å°æ³¢ç³»æ•° â†’ RCS
                    wavelet_coeffs = wavelet_transform.forward_transform(batch_rcs)
                    reconstructed_coeffs, latent = autoencoder(wavelet_coeffs)
                    reconstructed_rcs = wavelet_transform.inverse_transform(reconstructed_coeffs)
                else:
                    # ç›´æ¥æ¨¡å¼ï¼šRCS â†’ AE â†’ RCS
                    reconstructed_rcs, latent = autoencoder(batch_rcs)

                reconstruction_time = time.time() - start_time

                # æµ‹è¯•ç«¯åˆ°ç«¯é¢„æµ‹èƒ½åŠ›
                start_time = time.time()

                if mode == 'wavelet':
                    # å‚æ•° â†’ éšç©ºé—´ â†’ å°æ³¢ç³»æ•° â†’ RCS
                    predicted_latent = parameter_mapper(batch_params)
                    predicted_coeffs = autoencoder.decode(predicted_latent)
                    predicted_rcs = wavelet_transform.inverse_transform(predicted_coeffs)
                else:
                    # å‚æ•° â†’ éšç©ºé—´ â†’ RCS
                    predicted_latent = parameter_mapper(batch_params)
                    predicted_rcs = autoencoder.decode(predicted_latent)

                prediction_time = time.time() - start_time

                # è®¡ç®—è¯¯å·®
                recon_error = torch.mean((batch_rcs - reconstructed_rcs) ** 2).item()
                pred_error = torch.mean((batch_rcs - predicted_rcs) ** 2).item()

                reconstruction_errors.append(recon_error)
                prediction_errors.append(pred_error)
                latent_vectors.append(latent.cpu().numpy())
                inference_times.append(reconstruction_time + prediction_time)

        # åˆå¹¶ç»“æœ
        all_latents = np.concatenate(latent_vectors, axis=0)

        return {
            'reconstruction_mse': np.mean(reconstruction_errors),
            'reconstruction_std': np.std(reconstruction_errors),
            'prediction_mse': np.mean(prediction_errors),
            'prediction_std': np.std(prediction_errors),
            'latent_vectors': all_latents,
            'inference_time': np.mean(inference_times),
            'inference_time_std': np.std(inference_times),
            'mode': mode
        }

    def _calculate_comparison_metrics(self, wavelet_results: Dict[str, Any],
                                    direct_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å¯¹æ¯”æŒ‡æ ‡"""
        comparison = {}

        # é‡å»ºç²¾åº¦å¯¹æ¯”
        comparison['reconstruction_improvement'] = {
            'mse_ratio': wavelet_results['reconstruction_mse'] / direct_results['reconstruction_mse'],
            'better_mode': 'wavelet' if wavelet_results['reconstruction_mse'] < direct_results['reconstruction_mse'] else 'direct',
            'improvement_percent': abs(wavelet_results['reconstruction_mse'] - direct_results['reconstruction_mse']) /
                                 max(wavelet_results['reconstruction_mse'], direct_results['reconstruction_mse']) * 100
        }

        # é¢„æµ‹ç²¾åº¦å¯¹æ¯”
        comparison['prediction_improvement'] = {
            'mse_ratio': wavelet_results['prediction_mse'] / direct_results['prediction_mse'],
            'better_mode': 'wavelet' if wavelet_results['prediction_mse'] < direct_results['prediction_mse'] else 'direct',
            'improvement_percent': abs(wavelet_results['prediction_mse'] - direct_results['prediction_mse']) /
                                 max(wavelet_results['prediction_mse'], direct_results['prediction_mse']) * 100
        }

        # æ¨ç†é€Ÿåº¦å¯¹æ¯”
        comparison['speed_comparison'] = {
            'time_ratio': wavelet_results['inference_time'] / direct_results['inference_time'],
            'faster_mode': 'wavelet' if wavelet_results['inference_time'] < direct_results['inference_time'] else 'direct',
            'speedup_percent': abs(wavelet_results['inference_time'] - direct_results['inference_time']) /
                             max(wavelet_results['inference_time'], direct_results['inference_time']) * 100
        }

        return comparison

    def compare_feature_learning(self, test_data: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """
        ç‰¹å¾å­¦ä¹ èƒ½åŠ›å¯¹æ¯”

        Args:
            test_data: æµ‹è¯•æ•°æ®

        Returns:
            feature_comparison: ç‰¹å¾å­¦ä¹ å¯¹æ¯”ç»“æœ
        """
        print("ğŸ§  å¼€å§‹ç‰¹å¾å­¦ä¹ å¯¹æ¯”åˆ†æ...")

        # è·å–éšç©ºé—´è¡¨ç¤º
        wavelet_latents = self.comparison_results['performance']['wavelet_mode']['latent_vectors']
        direct_latents = self.comparison_results['performance']['direct_mode']['latent_vectors']

        results = {}

        # PCAåˆ†æ
        print("  ğŸ“Š PCAé™ç»´åˆ†æ...")
        results['pca_analysis'] = self._compare_pca(wavelet_latents, direct_latents)

        # t-SNEåˆ†æ
        print("  ğŸ“Š t-SNEèšç±»åˆ†æ...")
        results['tsne_analysis'] = self._compare_tsne(wavelet_latents, direct_latents)

        # ç‰¹å¾åˆ†å¸ƒåˆ†æ
        print("  ğŸ“Š ç‰¹å¾åˆ†å¸ƒåˆ†æ...")
        results['distribution_analysis'] = self._compare_distributions(wavelet_latents, direct_latents)

        self.comparison_results['feature_learning'] = results
        return results

    def _compare_pca(self, wavelet_latents: np.ndarray,
                    direct_latents: np.ndarray) -> Dict[str, Any]:
        """PCAå¯¹æ¯”åˆ†æ"""
        # å°æ³¢æ¨¡å¼PCA
        pca_wavelet = PCA(n_components=min(10, wavelet_latents.shape[1]))
        wavelet_pca = pca_wavelet.fit_transform(wavelet_latents)

        # ç›´æ¥æ¨¡å¼PCA
        pca_direct = PCA(n_components=min(10, direct_latents.shape[1]))
        direct_pca = pca_direct.fit_transform(direct_latents)

        return {
            'wavelet_explained_variance': pca_wavelet.explained_variance_ratio_,
            'direct_explained_variance': pca_direct.explained_variance_ratio_,
            'wavelet_cumulative_variance': np.cumsum(pca_wavelet.explained_variance_ratio_),
            'direct_cumulative_variance': np.cumsum(pca_direct.explained_variance_ratio_),
            'wavelet_pca_coords': wavelet_pca[:, :2],
            'direct_pca_coords': direct_pca[:, :2]
        }

    def _compare_tsne(self, wavelet_latents: np.ndarray,
                     direct_latents: np.ndarray) -> Dict[str, Any]:
        """t-SNEå¯¹æ¯”åˆ†æ"""
        # é™åˆ¶æ ·æœ¬æ•°é‡é¿å…è®¡ç®—è¿‡æ…¢
        max_samples = 50

        if len(wavelet_latents) > max_samples:
            indices = np.random.choice(len(wavelet_latents), max_samples, replace=False)
            wavelet_subset = wavelet_latents[indices]
            direct_subset = direct_latents[indices]
        else:
            wavelet_subset = wavelet_latents
            direct_subset = direct_latents

        # t-SNEé™ç»´
        perplexity = min(30, len(wavelet_subset) - 1)

        tsne_wavelet = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        wavelet_tsne = tsne_wavelet.fit_transform(wavelet_subset)

        tsne_direct = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        direct_tsne = tsne_direct.fit_transform(direct_subset)

        return {
            'wavelet_tsne_coords': wavelet_tsne,
            'direct_tsne_coords': direct_tsne
        }

    def _compare_distributions(self, wavelet_latents: np.ndarray,
                             direct_latents: np.ndarray) -> Dict[str, Any]:
        """ç‰¹å¾åˆ†å¸ƒå¯¹æ¯”"""
        return {
            'wavelet_mean': np.mean(wavelet_latents, axis=0),
            'wavelet_std': np.std(wavelet_latents, axis=0),
            'direct_mean': np.mean(direct_latents, axis=0),
            'direct_std': np.std(direct_latents, axis=0),
            'mean_difference': np.mean(np.abs(np.mean(wavelet_latents, axis=0) - np.mean(direct_latents, axis=0))),
            'std_difference': np.mean(np.abs(np.std(wavelet_latents, axis=0) - np.std(direct_latents, axis=0)))
        }

    def compare_computational_efficiency(self) -> Dict[str, Any]:
        """
        è®¡ç®—æ•ˆç‡å¯¹æ¯”

        Returns:
            efficiency_comparison: è®¡ç®—æ•ˆç‡å¯¹æ¯”ç»“æœ
        """
        print("âš¡ å¼€å§‹è®¡ç®—æ•ˆç‡å¯¹æ¯”åˆ†æ...")

        results = {}

        # æ¨¡å‹å¤æ‚åº¦å¯¹æ¯”
        results['model_complexity'] = self._compare_model_complexity()

        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        results['memory_usage'] = self._compare_memory_usage()

        # æ¨ç†æ—¶é—´å¯¹æ¯”ï¼ˆä»ä¹‹å‰çš„ç»“æœä¸­è·å–ï¼‰
        if 'performance' in self.comparison_results:
            results['inference_time'] = {
                'wavelet_time': self.comparison_results['performance']['wavelet_mode']['inference_time'],
                'direct_time': self.comparison_results['performance']['direct_mode']['inference_time'],
                'speedup_ratio': self.comparison_results['performance']['comparison']['speed_comparison']['time_ratio']
            }

        self.comparison_results['computational_efficiency'] = results
        return results

    def _compare_model_complexity(self) -> Dict[str, Any]:
        """æ¨¡å‹å¤æ‚åº¦å¯¹æ¯”"""
        # å°æ³¢ç³»ç»Ÿå‚æ•°
        wavelet_ae_params = sum(p.numel() for p in self.wavelet_system['autoencoder'].parameters())
        wavelet_mapper_params = sum(p.numel() for p in self.wavelet_system['parameter_mapper'].parameters())
        wavelet_total = wavelet_ae_params + wavelet_mapper_params

        # ç›´æ¥ç³»ç»Ÿå‚æ•°
        direct_ae_params = sum(p.numel() for p in self.direct_system['autoencoder'].parameters())
        direct_mapper_params = sum(p.numel() for p in self.direct_system['parameter_mapper'].parameters())
        direct_total = direct_ae_params + direct_mapper_params

        return {
            'wavelet_autoencoder_params': wavelet_ae_params,
            'wavelet_mapper_params': wavelet_mapper_params,
            'wavelet_total_params': wavelet_total,
            'direct_autoencoder_params': direct_ae_params,
            'direct_mapper_params': direct_mapper_params,
            'direct_total_params': direct_total,
            'parameter_ratio': wavelet_total / direct_total,
            'complexity_advantage': 'direct' if direct_total < wavelet_total else 'wavelet'
        }

    def _compare_memory_usage(self) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨å¯¹æ¯”ï¼ˆä¼°ç®—ï¼‰"""
        # åŸºäºæ¨¡å‹å‚æ•°å’Œä¸­é—´å˜é‡ä¼°ç®—

        # å°æ³¢æ¨¡å¼ï¼šéœ€è¦å­˜å‚¨å°æ³¢ç³»æ•°
        wavelet_intermediate = 91 * 91 * 8 * 4  # å°æ³¢ç³»æ•°å ç”¨ï¼ˆfloat32ï¼‰
        wavelet_base = sum(p.numel() * 4 for p in self.wavelet_system['autoencoder'].parameters())
        wavelet_total = wavelet_base + wavelet_intermediate

        # ç›´æ¥æ¨¡å¼ï¼šç›´æ¥å¤„ç†RCS
        direct_intermediate = 91 * 91 * 2 * 4  # RCSæ•°æ®å ç”¨ï¼ˆfloat32ï¼‰
        direct_base = sum(p.numel() * 4 for p in self.direct_system['autoencoder'].parameters())
        direct_total = direct_base + direct_intermediate

        return {
            'wavelet_memory_mb': wavelet_total / (1024 * 1024),
            'direct_memory_mb': direct_total / (1024 * 1024),
            'memory_ratio': wavelet_total / direct_total,
            'memory_advantage': 'direct' if direct_total < wavelet_total else 'wavelet'
        }

    def generate_comparison_report(self) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”åˆ†ææŠ¥å‘Š

        Returns:
            report: æ ¼å¼åŒ–çš„å¯¹æ¯”æŠ¥å‘Š
        """
        if not self.comparison_results:
            return "âŒ è¯·å…ˆè¿è¡Œå¯¹æ¯”åˆ†æï¼"

        report = []
        report.append("=" * 60)
        report.append("ğŸ”¬ AutoEncoderå¯¹æ¯”åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)

        # æ€§èƒ½å¯¹æ¯”
        if 'performance' in self.comparison_results:
            perf = self.comparison_results['performance']
            report.append("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
            report.append(f"  é‡å»ºç²¾åº¦ (MSE):")
            report.append(f"    å°æ³¢æ¨¡å¼: {perf['wavelet_mode']['reconstruction_mse']:.6f}")
            report.append(f"    ç›´æ¥æ¨¡å¼: {perf['direct_mode']['reconstruction_mse']:.6f}")
            report.append(f"    ä¼˜åŠ¿: {perf['comparison']['reconstruction_improvement']['better_mode']}")
            report.append(f"    æ”¹å–„: {perf['comparison']['reconstruction_improvement']['improvement_percent']:.1f}%")

            report.append(f"  é¢„æµ‹ç²¾åº¦ (MSE):")
            report.append(f"    å°æ³¢æ¨¡å¼: {perf['wavelet_mode']['prediction_mse']:.6f}")
            report.append(f"    ç›´æ¥æ¨¡å¼: {perf['direct_mode']['prediction_mse']:.6f}")
            report.append(f"    ä¼˜åŠ¿: {perf['comparison']['prediction_improvement']['better_mode']}")
            report.append(f"    æ”¹å–„: {perf['comparison']['prediction_improvement']['improvement_percent']:.1f}%")

        # è®¡ç®—æ•ˆç‡å¯¹æ¯”
        if 'computational_efficiency' in self.comparison_results:
            eff = self.comparison_results['computational_efficiency']
            report.append("\nâš¡ è®¡ç®—æ•ˆç‡å¯¹æ¯”:")
            report.append(f"  æ¨¡å‹å‚æ•°:")
            report.append(f"    å°æ³¢æ¨¡å¼: {eff['model_complexity']['wavelet_total_params']:,}")
            report.append(f"    ç›´æ¥æ¨¡å¼: {eff['model_complexity']['direct_total_params']:,}")
            report.append(f"    ä¼˜åŠ¿: {eff['model_complexity']['complexity_advantage']}")

            if 'inference_time' in eff:
                report.append(f"  æ¨ç†æ—¶é—´:")
                report.append(f"    å°æ³¢æ¨¡å¼: {eff['inference_time']['wavelet_time']:.4f}s")
                report.append(f"    ç›´æ¥æ¨¡å¼: {eff['inference_time']['direct_time']:.4f}s")
                report.append(f"    åŠ é€Ÿæ¯”: {eff['inference_time']['speedup_ratio']:.2f}x")

        # æ€»ç»“å»ºè®®
        report.append("\nğŸ¯ å»ºè®®:")
        report.append("  é«˜ç²¾åº¦è¦æ±‚ â†’ é€‰æ‹©è¡¨ç°æ›´å¥½çš„æ¨¡å¼")
        report.append("  å®æ—¶åº”ç”¨ â†’ é€‰æ‹©é€Ÿåº¦æ›´å¿«çš„æ¨¡å¼")
        report.append("  å†…å­˜å—é™ â†’ é€‰æ‹©å‚æ•°æ›´å°‘çš„æ¨¡å¼")

        return "\n".join(report)


def create_comparison_system(wavelet_system: Dict[str, Any],
                           direct_system: Dict[str, Any]) -> AutoEncoderComparator:
    """
    åˆ›å»ºAutoEncoderå¯¹æ¯”åˆ†æç³»ç»Ÿ

    Args:
        wavelet_system: å°æ³¢å¢å¼ºç³»ç»Ÿ
        direct_system: ç›´æ¥ç³»ç»Ÿ

    Returns:
        comparator: å¯¹æ¯”åˆ†æå™¨
    """
    return AutoEncoderComparator(wavelet_system, direct_system)