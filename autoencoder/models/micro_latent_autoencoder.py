"""
å¾®éšç©ºé—´AutoEncoderè®¾è®¡
ä¸“ä¸ºæå°éšç©ºé—´ç»´åº¦(å¦‚10ç»´)ä¼˜åŒ–çš„æ¶æ„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Dict, Any
import numpy as np


class MicroLatentAutoEncoder(nn.Module):
    """
    å¾®éšç©ºé—´AutoEncoder
    æ”¯æŒæå°éšç©ºé—´ç»´åº¦(10-64ç»´)çš„ä¼˜åŒ–è®¾è®¡
    """

    def __init__(self,
                 latent_dim: int = 10,
                 num_frequencies: int = 2,
                 wavelet_bands: int = 4,
                 dropout_rate: float = 0.1):
        """
        åˆå§‹åŒ–å¾®éšç©ºé—´AutoEncoder

        Args:
            latent_dim: éšç©ºé—´ç»´åº¦ (æ¨è10-64)
            num_frequencies: é¢‘ç‡æ•°é‡
            wavelet_bands: å°æ³¢é¢‘å¸¦æ•°
            dropout_rate: Dropoutæ¯”ç‡
        """
        super().__init__()

        self.latent_dim = latent_dim
        self.num_frequencies = num_frequencies
        self.wavelet_bands = wavelet_bands
        self.input_channels = num_frequencies * wavelet_bands  # 8
        self.dropout_rate = dropout_rate

        print(f"åˆå§‹åŒ–å¾®éšç©ºé—´CNN-AE: è¾“å…¥[{self.input_channels}, 49, 49] â†’ {latent_dim}ç»´éšç©ºé—´")

        # ===== CNNç¼–ç å™¨ (å¤ç”¨é«˜æ•ˆ3å±‚æ¶æ„) =====

        # Stage 1: [8, 49, 49] â†’ [64, 49, 49]
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.input_channels, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        # Stage 2: [64, 49, 49] â†’ [128, 25, 25]
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Dropout2d(dropout_rate)
        )

        # Stage 3: [128, 25, 25] â†’ [256, 13, 13]
        self.conv3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Dropout2d(dropout_rate)
        )

        # å…¨å±€å¹³å‡æ± åŒ– + æ¸è¿›å¼å‹ç¼©FCå±‚
        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc_features = 256 * 4 * 4  # 4096

        # ===== å…³é”®ï¼šæ¸è¿›å¼å‹ç¼©åˆ°å¾®éšç©ºé—´ =====
        self.encoder_fc = self._create_progressive_encoder()

        # ===== æ¸è¿›å¼æ‰©å¼ è§£ç å™¨ =====
        self.decoder_fc = self._create_progressive_decoder()

        # ===== CNNè§£ç å™¨ (å¤ç”¨æ¶æ„) =====

        # Stage 3 é€†å‘: [256, 4, 4] â†’ [128, 25, 25]
        self.deconv3 = nn.Sequential(
            nn.Unflatten(1, (256, 4, 4)),
            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=3, padding=0, output_padding=0),  # 4â†’13
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=0),  # 13â†’25
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )

        # Stage 2 é€†å‘: [128, 25, 25] â†’ [64, 49, 49]
        self.deconv2 = nn.Sequential(
            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=0),  # 25â†’49
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

        # Stage 1 é€†å‘: [64, 49, 49] â†’ [8, 49, 49]
        self.deconv1 = nn.Sequential(
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, self.input_channels, kernel_size=3, padding=1),
            nn.Tanh()  # å°æ³¢ç³»æ•°å¯ä»¥æœ‰è´Ÿå€¼
        )

        self._initialize_weights()

    def _create_progressive_encoder(self) -> nn.Module:
        """
        åˆ›å»ºæ¸è¿›å¼ç¼–ç å™¨ï¼Œé€æ­¥å‹ç¼©åˆ°å¾®éšç©ºé—´
        """
        layers = [nn.Flatten()]

        # æ ¹æ®éšç©ºé—´ç»´åº¦åŠ¨æ€è°ƒæ•´æ¶æ„
        current_dim = self.fc_features  # 4096

        if self.latent_dim <= 16:
            # æå°éšç©ºé—´ï¼šéœ€è¦æ›´å¤šå±‚æ¥å¹³æ»‘å‹ç¼©
            intermediate_dims = [1024, 256, 64]
            print(f"  æå°éšç©ºé—´è®¾è®¡: {current_dim} â†’ {' â†’ '.join(map(str, intermediate_dims))} â†’ {self.latent_dim}")
        elif self.latent_dim <= 64:
            # å°éšç©ºé—´ï¼šä¸­ç­‰å‹ç¼©
            intermediate_dims = [1024, 256]
            print(f"  å°éšç©ºé—´è®¾è®¡: {current_dim} â†’ {' â†’ '.join(map(str, intermediate_dims))} â†’ {self.latent_dim}")
        else:
            # æ­£å¸¸éšç©ºé—´ï¼šæ ‡å‡†å‹ç¼©
            intermediate_dims = [512]
            print(f"  æ ‡å‡†éšç©ºé—´è®¾è®¡: {current_dim} â†’ {' â†’ '.join(map(str, intermediate_dims))} â†’ {self.latent_dim}")

        # æ·»åŠ ä¸­é—´å±‚
        for dim in intermediate_dims:
            layers.extend([
                nn.Linear(current_dim, dim),
                nn.ReLU(inplace=True),
                nn.Dropout(self.dropout_rate)
            ])
            current_dim = dim

        # æœ€ç»ˆå‹ç¼©åˆ°éšç©ºé—´
        layers.append(nn.Linear(current_dim, self.latent_dim))

        return nn.Sequential(*layers)

    def _create_progressive_decoder(self) -> nn.Module:
        """
        åˆ›å»ºæ¸è¿›å¼è§£ç å™¨ï¼Œä»å¾®éšç©ºé—´é€æ­¥æ‰©å¼ 
        """
        layers = []

        # æ ¹æ®éšç©ºé—´ç»´åº¦åŠ¨æ€è°ƒæ•´æ¶æ„ï¼ˆç¼–ç å™¨çš„é€†å‘ï¼‰
        current_dim = self.latent_dim

        if self.latent_dim <= 16:
            # æå°éšç©ºé—´ï¼šéœ€è¦æ›´å¤šå±‚æ¥å¹³æ»‘æ‰©å¼ 
            intermediate_dims = [64, 256, 1024]
        elif self.latent_dim <= 64:
            # å°éšç©ºé—´ï¼šä¸­ç­‰æ‰©å¼ 
            intermediate_dims = [256, 1024]
        else:
            # æ­£å¸¸éšç©ºé—´ï¼šæ ‡å‡†æ‰©å¼ 
            intermediate_dims = [512]

        # æ·»åŠ ä¸­é—´å±‚
        for dim in intermediate_dims:
            layers.extend([
                nn.Linear(current_dim, dim),
                nn.ReLU(inplace=True),
                nn.Dropout(self.dropout_rate)
            ])
            current_dim = dim

        # æœ€ç»ˆæ‰©å¼ åˆ°ç‰¹å¾ç©ºé—´
        layers.extend([
            nn.Linear(current_dim, self.fc_features),
            nn.ReLU(inplace=True)
        ])

        return nn.Sequential(*layers)

    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # å¯¹äºæå°éšç©ºé—´ï¼Œä½¿ç”¨æ›´å°çš„åˆå§‹åŒ–æ–¹å·®
                if self.latent_dim <= 16:
                    nn.init.normal_(m.weight, 0, 0.005)
                else:
                    nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """ç¼–ç å™¨å‰å‘ä¼ æ’­"""
        # è¾“å…¥éªŒè¯
        if x.shape[1:] != (8, 49, 49):
            raise ValueError(f"æœŸæœ›è¾“å…¥[B, 8, 49, 49], å¾—åˆ°{x.shape}")

        # CNNç¼–ç 
        x1 = self.conv1(x)      # [B, 64, 49, 49]
        x2 = self.conv2(x1)     # [B, 128, 25, 25]
        x3 = self.conv3(x2)     # [B, 256, 13, 13]

        # å…¨å±€æ± åŒ–å¹¶æ¸è¿›å¼å‹ç¼©
        x_pool = self.global_pool(x3)  # [B, 256, 4, 4]
        latent = self.encoder_fc(x_pool)  # [B, latent_dim]

        return latent

    def decode(self, latent: torch.Tensor) -> torch.Tensor:
        """è§£ç å™¨å‰å‘ä¼ æ’­"""
        # æ¸è¿›å¼æ‰©å¼ 
        x = self.decoder_fc(latent)     # [B, 4096]

        # CNNè§£ç 
        x = self.deconv3(x)             # [B, 128, 25, 25]
        x = self.deconv2(x)             # [B, 64, 49, 49]
        x = self.deconv1(x)             # [B, 8, 49, 49]

        return x

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        latent = self.encode(x)
        reconstructed = self.decode(latent)
        return reconstructed, latent

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        # è®¡ç®—å‹ç¼©æ¯”
        input_size = 8 * 49 * 49  # 19208
        compression_ratio = input_size / self.latent_dim

        return {
            'model_name': f'Micro-Latent AutoEncoder ({self.latent_dim}D)',
            'architecture': 'Progressive compression for micro latent space',
            'input_shape': f'[batch, {self.input_channels}, 49, 49]',
            'latent_dim': self.latent_dim,
            'compression_ratio': f'{compression_ratio:.1f}x',
            'total_params': total_params,
            'trainable_params': trainable_params,
            'encoder_path': self._get_encoder_path(),
            'decoder_path': self._get_decoder_path(),
            'advantages': [
                'æ¸è¿›å¼å‹ç¼©ï¼Œé¿å…ä¿¡æ¯éª¤é™',
                'æ”¯æŒæå°éšç©ºé—´(10ç»´)',
                'åŠ¨æ€æ¶æ„é€‚åº”ä¸åŒå‹ç¼©æ¯”',
                'ä¼˜åŒ–çš„æƒé‡åˆå§‹åŒ–',
                'é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±'
            ]
        }

    def _get_encoder_path(self) -> str:
        """è·å–ç¼–ç å™¨è·¯å¾„æè¿°"""
        if self.latent_dim <= 16:
            return "4096â†’1024â†’256â†’64â†’latent"
        elif self.latent_dim <= 64:
            return "4096â†’1024â†’256â†’latent"
        else:
            return "4096â†’512â†’latent"

    def _get_decoder_path(self) -> str:
        """è·å–è§£ç å™¨è·¯å¾„æè¿°"""
        if self.latent_dim <= 16:
            return "latentâ†’64â†’256â†’1024â†’4096"
        elif self.latent_dim <= 64:
            return "latentâ†’256â†’1024â†’4096"
        else:
            return "latentâ†’512â†’4096"


def test_micro_latent_dimensions():
    """æµ‹è¯•ä¸åŒå¾®éšç©ºé—´ç»´åº¦"""
    print("=== æµ‹è¯•å¾®éšç©ºé—´AutoEncoder ===")
    print()

    test_dims = [10, 16, 32, 64, 128, 256]
    test_input = torch.randn(2, 8, 49, 49)

    results = []

    for dim in test_dims:
        print(f"æµ‹è¯• {dim}ç»´éšç©ºé—´:")

        model = MicroLatentAutoEncoder(latent_dim=dim)

        with torch.no_grad():
            reconstructed, latent = model(test_input)
            mse = torch.mean((test_input - reconstructed)**2).item()

        info = model.get_model_info()

        print(f"  å‹ç¼©æ¯”: {info['compression_ratio']}")
        print(f"  ç¼–ç è·¯å¾„: {info['encoder_path']}")
        print(f"  å‚æ•°é‡: {info['total_params']:,}")
        print(f"  é‡å»ºMSE: {mse:.6f}")
        print()

        results.append({
            'dim': dim,
            'compression': info['compression_ratio'],
            'params': info['total_params'],
            'mse': mse
        })

    print("ğŸ“Š ç»´åº¦å¯¹æ¯”æ€»ç»“:")
    print("ç»´åº¦  å‹ç¼©æ¯”    å‚æ•°é‡      é‡å»ºMSE")
    print("-" * 40)
    for r in results:
        print(f"{r['dim']:3d}   {r['compression']:8s} {r['params']:8,}  {r['mse']:.6f}")

    return results


def compare_with_standard_ae():
    """ä¸æ ‡å‡†AutoEncoderå¯¹æ¯”"""
    print("\n=== å¾®éšç©ºé—´ vs æ ‡å‡†æ¶æ„å¯¹æ¯” ===")

    from autoencoder.models.efficient_cnn_autoencoder import EfficientCNNAutoEncoder

    # åˆ›å»ºæ¨¡å‹
    micro_ae = MicroLatentAutoEncoder(latent_dim=10)
    standard_ae = EfficientCNNAutoEncoder(latent_dim=10)

    # å‚æ•°ç»Ÿè®¡
    micro_params = sum(p.numel() for p in micro_ae.parameters())
    standard_params = sum(p.numel() for p in standard_ae.parameters())

    print(f"å¾®éšç©ºé—´AEå‚æ•°: {micro_params:,}")
    print(f"æ ‡å‡†AEå‚æ•°:     {standard_params:,}")
    print(f"å‚æ•°å‡å°‘:       {(standard_params-micro_params)/standard_params*100:.1f}%")

    # æµ‹è¯•é‡å»ºè´¨é‡
    test_input = torch.randn(4, 8, 49, 49)

    with torch.no_grad():
        micro_recon, micro_latent = micro_ae(test_input)
        standard_recon, standard_latent = standard_ae(test_input)

        micro_mse = torch.mean((test_input - micro_recon)**2).item()
        standard_mse = torch.mean((test_input - standard_recon)**2).item()

    print(f"å¾®éšç©ºé—´é‡å»ºMSE: {micro_mse:.6f}")
    print(f"æ ‡å‡†æ¶æ„é‡å»ºMSE: {standard_mse:.6f}")

    return micro_ae, standard_ae


if __name__ == "__main__":
    # æµ‹è¯•ä¸åŒéšç©ºé—´ç»´åº¦
    test_micro_latent_dimensions()

    # å¯¹æ¯”æ ‡å‡†æ¶æ„
    compare_with_standard_ae()