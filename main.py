"""
RCSå°æ³¢ç¥ç»ç½‘ç»œä¸»ç¨‹åº

æä¾›å¤šç§è¿è¡Œæ¨¡å¼:
1. GUIæ¨¡å¼: å›¾å½¢ç•Œé¢æ“ä½œ
2. è®­ç»ƒæ¨¡å¼: å‘½ä»¤è¡Œè®­ç»ƒ
3. è¯„ä¼°æ¨¡å¼: æ¨¡å‹è¯„ä¼°
4. é¢„æµ‹æ¨¡å¼: å•æ¬¡é¢„æµ‹
5. æ‰¹å¤„ç†æ¨¡å¼: æ‰¹é‡å¤„ç†

ä½¿ç”¨è¯´æ˜:
python main.py --mode gui                    # å¯åŠ¨GUIç•Œé¢
python main.py --mode train                  # å‘½ä»¤è¡Œè®­ç»ƒ
python main.py --mode evaluate               # æ¨¡å‹è¯„ä¼°
python main.py --mode predict                # å•æ¬¡é¢„æµ‹
python main.py --mode batch                  # æ‰¹é‡å¤„ç†

ä½œè€…: RCS Wavelet Network Project
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import argparse
import json
import numpy as np
import torch
from datetime import datetime
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    import rcs_data_reader as rdr
    import rcs_visual as rv
    from wavelet_network import create_model, create_loss_function, TriDimensionalRCSNet
    from training import (CrossValidationTrainer, RCSDataLoader,
                         create_training_config, create_data_config, RCSDataset)
    from evaluation import RCSEvaluator, evaluate_model_with_visualizations
    from gui import RCSWaveletGUI
except ImportError as e:
    print(f"æ¨¡å—å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–æ¨¡å—éƒ½å·²æ­£ç¡®å®‰è£…")
    sys.exit(1)

warnings.filterwarnings('ignore')


class RCSWaveletApp:
    """
    RCSå°æ³¢ç½‘ç»œåº”ç”¨ä¸»ç±»
    """

    def __init__(self):
        """åˆå§‹åŒ–åº”ç”¨"""
        self.config = self.load_config()
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.create_directories()

    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = 'config.json'

        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"é…ç½®æ–‡ä»¶å·²åŠ è½½: {config_file}")
                return config
            except Exception as e:
                print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")

        # ä½¿ç”¨é»˜è®¤é…ç½®
        print("ä½¿ç”¨é»˜è®¤é…ç½®")
        return self.create_default_config()

    def create_default_config(self):
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        config = {
            "data": {
                "params_file": r"..\parameter\parameters_sorted.csv",
                "rcs_data_dir": r"..\parameter\csv_output",
                "model_ids": [f"{i:03d}" for i in range(1, 101)],
                "frequencies": ["1.5G", "3G"]
            },
            "model": {
                "input_dim": 9,
                "hidden_dims": [128, 256],
                "dropout_rate": 0.2
            },
            "training": {
                "batch_size": 8,
                "learning_rate": 1e-3,
                "weight_decay": 1e-4,
                "epochs": 200,
                "early_stopping_patience": 20,
                "use_cross_validation": True,
                "n_folds": 5,
                "loss_weights": {
                    "mse": 1.0,
                    "symmetry": 0.1,
                    "multiscale": 0.2
                }
            },
            "evaluation": {
                "test_split": 0.2,
                "metrics": ["rmse", "r2", "correlation", "physics_consistency"],
                "visualize_samples": 5
            },
            "output": {
                "model_dir": "models",
                "results_dir": "results",
                "logs_dir": "logs",
                "visualizations_dir": "visualizations"
            }
        }

        # ä¿å­˜é»˜è®¤é…ç½®
        self.save_config(config)
        return config

    def save_config(self, config):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            with open('config.json', 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print("é…ç½®æ–‡ä»¶å·²ä¿å­˜: config.json")
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")

    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        dirs = [
            self.config['output']['model_dir'],
            self.config['output']['results_dir'],
            self.config['output']['logs_dir'],
            self.config['output']['visualizations_dir'],
            'checkpoints'
        ]

        for dir_path in dirs:
            os.makedirs(dir_path, exist_ok=True)

    def run_gui(self):
        """è¿è¡ŒGUIæ¨¡å¼"""
        print("å¯åŠ¨GUIç•Œé¢...")

        try:
            # å½»åº•ä¿®å¤tkinterç¯å¢ƒå†²çª
            import os
            import sys

            # 1. å®Œå…¨æ¸…ç†å†²çªçš„ç¯å¢ƒå˜é‡
            env_vars_to_clear = [
                'TCL_LIBRARY', 'TK_LIBRARY', 'TCLLIBPATH',
                'TCLLIBPATH', 'TIX_LIBRARY'
            ]
            for var in env_vars_to_clear:
                if var in os.environ:
                    print(f"æ¸…ç†ç¯å¢ƒå˜é‡: {var}")
                    del os.environ[var]

            # 2. å¼ºåˆ¶ä½¿ç”¨å½“å‰Pythonç¯å¢ƒçš„Tcl/Tk
            python_dir = os.path.dirname(sys.executable)

            # è®¾ç½®æ­£ç¡®çš„Tcl/Tkè·¯å¾„
            tcl_lib_paths = [
                os.path.join(python_dir, "tcl", "tcl8.6"),
                os.path.join(python_dir, "lib", "tcl8.6"),
                os.path.join(python_dir, "Library", "lib", "tcl8.6"),
                os.path.join(os.path.dirname(python_dir), "Library", "lib", "tcl8.6")
            ]

            tk_lib_paths = [
                os.path.join(python_dir, "tcl", "tk8.6"),
                os.path.join(python_dir, "lib", "tk8.6"),
                os.path.join(python_dir, "Library", "lib", "tk8.6"),
                os.path.join(os.path.dirname(python_dir), "Library", "lib", "tk8.6")
            ]

            # è®¾ç½®TCL_LIBRARY
            for tcl_path in tcl_lib_paths:
                if os.path.exists(tcl_path):
                    os.environ['TCL_LIBRARY'] = tcl_path
                    print(f"è®¾ç½®TCL_LIBRARY: {tcl_path}")
                    break

            # è®¾ç½®TK_LIBRARY
            for tk_path in tk_lib_paths:
                if os.path.exists(tk_path):
                    os.environ['TK_LIBRARY'] = tk_path
                    print(f"è®¾ç½®TK_LIBRARY: {tk_path}")
                    break

            # 3. å¯¼å…¥tkinter
            import tkinter as tk
            print("tkinterå¯¼å…¥æˆåŠŸ")

            # 4. åˆ›å»ºGUI
            root = tk.Tk()

            # è®¾ç½®çª—å£å±æ€§
            root.title("RCSå°æ³¢ç¥ç»ç½‘ç»œ - å¢å¼ºç‰ˆ (åŒæ¨¡å¼AutoEncoder + å°æ³¢åˆ†æ)")
            root.geometry("1400x900")

            # åˆ›å»ºä¸»GUIå®ä¾‹
            app = RCSWaveletGUI(root)
            print("GUIåˆ›å»ºæˆåŠŸ")

            # 5. é›†æˆAutoEncoderæ‰©å±•åŠŸèƒ½
            try:
                from gui_autoencoder_extension import integrate_extension_to_gui
                print("æ­£åœ¨é›†æˆAutoEncoderæ‰©å±•...")
                extension = integrate_extension_to_gui(app)
                print("âœ… AutoEncoderæ‰©å±•åŠŸèƒ½é›†æˆæˆåŠŸ")

                # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
                startup_message = '''ğŸŠ å¢å¼ºç‰ˆGUIå¯åŠ¨æˆåŠŸï¼

æ–°å¢åŠŸèƒ½:
âœ¨ åŒæ¨¡å¼AutoEncoderæ”¯æŒ
âœ¨ æ€§èƒ½å¯¹æ¯”åˆ†æ
âœ¨ å°æ³¢å˜æ¢å¯è§†åŒ–
âœ¨ ä¼˜åŒ–çš„ç”¨æˆ·ç•Œé¢

ä½¿ç”¨æ­¥éª¤:
1. åˆ‡æ¢åˆ°AutoEncoderæ ‡ç­¾é¡µ
2. é€‰æ‹©æ¨¡å¼ (å°æ³¢å¢å¼º/ç›´æ¥æ¨¡å¼)
3. åŠ è½½æ•°æ®å¹¶åˆ›å»ºç³»ç»Ÿ
4. è¿è¡Œåˆ†æå’Œå¯¹æ¯”

äº«å—ä½¿ç”¨å§ï¼ğŸš€'''

                app.log_message(startup_message)

            except ImportError as ext_e:
                print(f"âš ï¸ AutoEncoderæ‰©å±•åŠ è½½å¤±è´¥: {ext_e}")
                print("å°†ä½¿ç”¨åŸºç¡€GUIç‰ˆæœ¬")

            print("å¯åŠ¨ä¸»å¾ªç¯...")
            root.mainloop()
        except ImportError:
            print("é”™è¯¯: tkinteræœªå®‰è£…ï¼Œæ— æ³•å¯åŠ¨GUI")
            print("å»ºè®®ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼ï¼š")
            print("  python quick_test.py  # åŠŸèƒ½æµ‹è¯•")
            print("  python main.py --mode train  # è®­ç»ƒæ¨¡å‹")
            return False
        except Exception as e:
            print(f"GUIå¯åŠ¨å¤±è´¥: {e}")
            print("å»ºè®®ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼ï¼š")
            print("  python quick_test.py  # åŠŸèƒ½æµ‹è¯•")
            print("  python main.py --mode train  # è®­ç»ƒæ¨¡å‹")
            return False

        return True

    def run_training(self, args):
        """è¿è¡Œè®­ç»ƒæ¨¡å¼"""
        print("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

        try:
            # åŠ è½½æ•°æ®
            print("åŠ è½½è®­ç»ƒæ•°æ®...")
            data_loader = RCSDataLoader(self.config['data'])
            param_data, rcs_data = data_loader.load_data()

            print(f"æ•°æ®åŠ è½½å®Œæˆ: å‚æ•° {param_data.shape}, RCS {rcs_data.shape}")

            # è·å–preprocessing_statsï¼ˆå¦‚æœæ•°æ®åŠ è½½å™¨æœ‰çš„è¯ï¼‰
            preprocessing_stats = getattr(data_loader, 'preprocessing_stats', None)
            use_log_output = getattr(data_loader, 'use_log_preprocessing', False)

            # åˆ›å»ºæ•°æ®é›†
            dataset = RCSDataset(param_data, rcs_data, augment=True)

            # é…ç½®è®­ç»ƒå‚æ•°
            training_config = self.config['training'].copy()
            if args.epochs:
                training_config['epochs'] = args.epochs
            if args.batch_size:
                training_config['batch_size'] = args.batch_size
            if args.learning_rate:
                training_config['learning_rate'] = args.learning_rate
            if args.patience:
                training_config['early_stopping_patience'] = args.patience

            # å¼€å§‹è®­ç»ƒ
            if training_config['use_cross_validation']:
                print("ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒ...")
                trainer = CrossValidationTrainer(
                    self.config['model'],
                    device=self.device,
                    n_folds=training_config['n_folds']
                )

                results = trainer.cross_validate(dataset, training_config)

                print(f"äº¤å‰éªŒè¯å®Œæˆ:")
                print(f"å¹³å‡å¾—åˆ†: {results['mean_score']:.4f} Â± {results['std_score']:.4f}")
                print(f"æœ€ä½³fold: {results['best_fold']}")

                # ä¿å­˜ç»“æœ
                results_file = os.path.join(
                    self.config['output']['results_dir'],
                    f"cv_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                )

                with open(results_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, default=str)

                print(f"è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {results_file}")

            else:
                print("ä½¿ç”¨ç®€å•è®­ç»ƒæ¨¡å¼...")
                from torch.utils.data import random_split, DataLoader as TorchDataLoader
                import torch.optim as optim

                # åˆ†å‰²æ•°æ®é›†ï¼ˆä½¿ç”¨å›ºå®šç§å­ç¡®ä¿å¯é‡ç°ï¼‰
                import torch
                torch.manual_seed(42)

                train_size = int(len(dataset) * 0.8)
                val_size = len(dataset) - train_size

                # ä½¿ç”¨å›ºå®šç§å­çš„ç”Ÿæˆå™¨
                generator = torch.Generator().manual_seed(42)
                train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)

                print(f"æ•°æ®åˆ†å‰²: è®­ç»ƒé›† {train_size} æ ·æœ¬, éªŒè¯é›† {val_size} æ ·æœ¬")

                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                # ä¸ºè®­ç»ƒDataLoaderè®¾ç½®å›ºå®šç§å­
                train_generator = torch.Generator().manual_seed(42)

                train_loader = TorchDataLoader(train_dataset,
                                             batch_size=training_config['batch_size'],
                                             shuffle=True,
                                             generator=train_generator)  # å›ºå®šç§å­
                val_loader = TorchDataLoader(val_dataset,
                                           batch_size=training_config['batch_size'],
                                           shuffle=False)

                # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
                from training import ProgressiveTrainer
                model = create_model(**self.config['model'])
                trainer = ProgressiveTrainer(model, self.device)

                # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
                optimizer = optim.Adam(model.parameters(),
                                     lr=training_config['learning_rate'],
                                     weight_decay=training_config['weight_decay'])

                scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode='min', factor=0.5, patience=10
                )

                # åˆ›å»ºæŸå¤±å‡½æ•°
                loss_fn = create_loss_function(loss_weights=training_config.get('loss_weights'))

                # è®­ç»ƒå¾ªç¯
                best_val_loss = float('inf')
                patience_counter = 0

                for epoch in range(training_config['epochs']):
                    # è®­ç»ƒ
                    train_losses = trainer.train_epoch(train_loader, optimizer, loss_fn,
                                                     epoch, training_config['epochs'])

                    # éªŒè¯
                    val_losses = trainer.validate_epoch(val_loader, loss_fn)

                    # å­¦ä¹ ç‡è°ƒåº¦
                    scheduler.step(val_losses['total'])

                    # è®°å½•è¿›åº¦
                    if epoch % 10 == 0:
                        print(f"Epoch {epoch+1}/{training_config['epochs']}: "
                              f"Train Loss: {train_losses['total']:.4f}, "
                              f"Val Loss: {val_losses['total']:.4f}")

                    # æ—©åœæ£€æŸ¥
                    if val_losses['total'] < best_val_loss:
                        best_val_loss = val_losses['total']
                        patience_counter = 0

                        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå®Œæ•´çš„checkpointæ ¼å¼ï¼‰
                        os.makedirs('checkpoints', exist_ok=True)

                        # åˆ›å»ºå®Œæ•´çš„checkpointï¼ŒåŒ…å«preprocessing_stats
                        checkpoint = {
                            'model_state_dict': model.state_dict(),
                            'preprocessing_stats': preprocessing_stats,
                            'use_log_output': use_log_output,
                            'epoch': epoch,
                            'val_loss': best_val_loss
                        }
                        torch.save(checkpoint, 'checkpoints/best_model_simple.pth')

                        if preprocessing_stats:
                            print(f"  å·²ä¿å­˜preprocessing_stats: mean={preprocessing_stats['mean']:.2f}, std={preprocessing_stats['std']:.2f}")
                        else:
                            print("  è­¦å‘Š: æ— preprocessing_statsä¿¡æ¯")
                    else:
                        patience_counter += 1

                    if patience_counter >= training_config['early_stopping_patience']:
                        print(f"æ—©åœäºepoch {epoch+1}")
                        break

                print(f"ç®€å•è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                print("æ¨¡å‹å·²ä¿å­˜åˆ°: checkpoints/best_model_simple.pth")

        except Exception as e:
            print(f"è®­ç»ƒå¤±è´¥: {e}")
            return False

        return True

    def run_evaluation(self, args):
        """è¿è¡Œè¯„ä¼°æ¨¡å¼"""
        print("å¼€å§‹æ¨¡å‹è¯„ä¼°...")

        if not args.model_path or not os.path.exists(args.model_path):
            print("é”™è¯¯: è¯·æŒ‡å®šæœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„")
            return False

        try:
            # åŠ è½½æ•°æ®
            data_loader = RCSDataLoader(self.config['data'])
            param_data, rcs_data = data_loader.load_data()

            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
            test_size = int(len(param_data) * self.config['evaluation']['test_split'])
            test_dataset = RCSDataset(
                param_data[-test_size:],
                rcs_data[-test_size:],
                augment=False
            )

            # æ‰§è¡Œè¯„ä¼°
            output_dir = os.path.join(
                self.config['output']['results_dir'],
                f"evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )

            results = evaluate_model_with_visualizations(
                args.model_path,
                test_dataset,
                self.config['model'],
                output_dir
            )

            print(f"è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_dir}")

        except Exception as e:
            print(f"è¯„ä¼°å¤±è´¥: {e}")
            return False

        return True

    def run_prediction(self, args):
        """è¿è¡Œé¢„æµ‹æ¨¡å¼"""
        print("å¼€å§‹RCSé¢„æµ‹...")

        if not args.model_path or not os.path.exists(args.model_path):
            print("é”™è¯¯: è¯·æŒ‡å®šæœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„")
            return False

        if not args.params:
            print("é”™è¯¯: è¯·æŒ‡å®šè¾“å…¥å‚æ•°")
            return False

        try:
            # åŠ è½½æ¨¡å‹
            model = create_model(**self.config['model'])
            model.load_state_dict(torch.load(args.model_path, map_location=self.device))
            model.to(self.device)
            model.eval()

            # å¤„ç†è¾“å…¥å‚æ•°
            if isinstance(args.params, str):
                # ä»æ–‡ä»¶åŠ è½½å‚æ•°
                if os.path.exists(args.params):
                    params = np.loadtxt(args.params, delimiter=',')
                else:
                    # è§£æå­—ç¬¦ä¸²å‚æ•°
                    params = np.array([float(x) for x in args.params.split(',')])
            else:
                params = np.array(args.params)

            # ç¡®ä¿å‚æ•°å½¢çŠ¶æ­£ç¡®
            if params.ndim == 1:
                params = params.reshape(1, -1)

            # æ‰§è¡Œé¢„æµ‹
            with torch.no_grad():
                params_tensor = torch.tensor(params, dtype=torch.float32).to(self.device)
                prediction = model(params_tensor)
                prediction = prediction.cpu().numpy()

            # ä¿å­˜é¢„æµ‹ç»“æœ
            output_file = args.output or f"prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.npz"

            np.savez(output_file,
                    prediction=prediction,
                    input_params=params,
                    frequencies=['1.5GHz', '3GHz'])

            print(f"é¢„æµ‹å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
            print(f"é¢„æµ‹å½¢çŠ¶: {prediction.shape}")

            # ç”Ÿæˆå¯è§†åŒ–ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if args.visualize:
                self._visualize_prediction(prediction[0], output_file.replace('.npz', ''))

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥: {e}")
            return False

        return True

    def run_batch_processing(self, args):
        """è¿è¡Œæ‰¹å¤„ç†æ¨¡å¼"""
        print("å¼€å§‹æ‰¹é‡å¤„ç†...")

        if not args.input_dir or not os.path.exists(args.input_dir):
            print("é”™è¯¯: è¯·æŒ‡å®šæœ‰æ•ˆçš„è¾“å…¥ç›®å½•")
            return False

        try:
            # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
            input_files = []
            for ext in ['*.csv', '*.txt', '*.npz']:
                import glob
                input_files.extend(glob.glob(os.path.join(args.input_dir, ext)))

            print(f"æ‰¾åˆ° {len(input_files)} ä¸ªè¾“å…¥æ–‡ä»¶")

            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            for file_path in input_files:
                print(f"å¤„ç†æ–‡ä»¶: {file_path}")
                # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„æ‰¹å¤„ç†é€»è¾‘

            print("æ‰¹é‡å¤„ç†å®Œæˆ")

        except Exception as e:
            print(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            return False

        return True

    def _visualize_prediction(self, prediction, output_prefix):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        try:
            import matplotlib.pyplot as plt

            fig, axes = plt.subplots(1, 2, figsize=(12, 5))

            # å®šä¹‰è§’åº¦èŒƒå›´ (åŸºäºå®é™…æ•°æ®)
            phi_range = (-45.0, 45.0)  # Ï†èŒƒå›´: -45Â° åˆ° +45Â°
            theta_range = (45.0, 135.0)  # Î¸èŒƒå›´: 45Â° åˆ° 135Â°
            extent = [phi_range[0], phi_range[1], theta_range[1], theta_range[0]]

            # 1.5GHz
            im1 = axes[0].imshow(prediction[:, :, 0], cmap='jet', aspect='equal', extent=extent)
            axes[0].set_title('1.5GHz RCSé¢„æµ‹')
            axes[0].set_xlabel('Ï† (æ–¹ä½è§’, åº¦)')
            axes[0].set_ylabel('Î¸ (ä¿¯ä»°è§’, åº¦)')
            plt.colorbar(im1, ax=axes[0])

            # 3GHz
            im2 = axes[1].imshow(prediction[:, :, 1], cmap='jet', aspect='equal', extent=extent)
            axes[1].set_title('3GHz RCSé¢„æµ‹')
            axes[1].set_xlabel('Ï† (æ–¹ä½è§’, åº¦)')
            axes[1].set_ylabel('Î¸ (ä¿¯ä»°è§’, åº¦)')
            plt.colorbar(im2, ax=axes[1])

            plt.tight_layout()

            vis_file = f"{output_prefix}_visualization.png"
            plt.savefig(vis_file, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {vis_file}")

        except Exception as e:
            print(f"å¯è§†åŒ–å¤±è´¥: {e}")


def create_argument_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="RCSå°æ³¢ç¥ç»ç½‘ç»œé¢„æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py --mode gui                              # å¯åŠ¨GUIç•Œé¢
  python main.py --mode train --epochs 100              # è®­ç»ƒ100è½®
  python main.py --mode train --epochs 100 --patience 30 # è®­ç»ƒ100è½®ï¼Œæ—©åœè€å¿ƒ30è½®
  python main.py --mode evaluate --model models/best.pth # è¯„ä¼°æ¨¡å‹
  python main.py --mode predict --model models/best.pth --params "1,2,3,4,5,6,7,8,9"
  python main.py --mode batch --input-dir data/         # æ‰¹é‡å¤„ç†
        """
    )

    parser.add_argument('--mode',
                       choices=['gui', 'train', 'evaluate', 'predict', 'batch'],
                       default='gui',
                       help='è¿è¡Œæ¨¡å¼ (é»˜è®¤: gui)')

    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning-rate', type=float, help='å­¦ä¹ ç‡')
    parser.add_argument('--patience', type=int, help='æ—©åœè€å¿ƒå€¼ï¼Œè¿ç»­å¤šå°‘è½®éªŒè¯æŸå¤±ä¸æ”¹å–„å°±åœæ­¢è®­ç»ƒ')

    # è¯„ä¼°å’Œé¢„æµ‹ç›¸å…³å‚æ•°
    parser.add_argument('--model-path', help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--params', help='è¾“å…¥å‚æ•° (é€—å·åˆ†éš”æˆ–æ–‡ä»¶è·¯å¾„)')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--visualize', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–ç»“æœ')

    # æ‰¹å¤„ç†ç›¸å…³å‚æ•°
    parser.add_argument('--input-dir', help='è¾“å…¥ç›®å½•è·¯å¾„')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')

    return parser


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("RCSå°æ³¢ç¥ç»ç½‘ç»œé¢„æµ‹ç³»ç»Ÿ v1.0")
    print("ä½œè€…: RCS Wavelet Network Project")
    print("=" * 60)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = create_argument_parser()
    args = parser.parse_args()

    # åˆ›å»ºåº”ç”¨å®ä¾‹
    app = RCSWaveletApp()

    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œé‡æ–°åŠ è½½é…ç½®
    if args.config and os.path.exists(args.config):
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                app.config = json.load(f)
            print(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        except Exception as e:
            print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")

    # æ ¹æ®æ¨¡å¼è¿è¡Œç›¸åº”åŠŸèƒ½
    success = False

    if args.mode == 'gui':
        success = app.run_gui()

    elif args.mode == 'train':
        success = app.run_training(args)

    elif args.mode == 'evaluate':
        success = app.run_evaluation(args)

    elif args.mode == 'predict':
        success = app.run_prediction(args)

    elif args.mode == 'batch':
        success = app.run_batch_processing(args)

    # è¾“å‡ºç»“æœ
    if success:
        print("\nç¨‹åºæ‰§è¡ŒæˆåŠŸå®Œæˆ")
        sys.exit(0)
    else:
        print("\nç¨‹åºæ‰§è¡Œå¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()